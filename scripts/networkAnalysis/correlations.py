# -*- coding: utf-8 -*-
"""correlations_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fhkzuDGyxsMQmVQKGyY2T3OhvHNK1LYn
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import geopandas as gpd
import itertools
import plotly.graph_objects as go
import json

from .helperFunc import *

#**************************************************************************
def correlations(eventDate=20131002, window_size=15, wavelet_transform='modwt',startTime=0,endTime=1440):
    # Get the current working directory + desired directories
    main_dir, event_dir, output_dir = get_paths(eventDate)

    # Create output directory if none exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"The directory {output_dir} has been created!")

    # Location Dataframe for Devices (e.g Latitude / Longitude )
    loc_file = event_dir + '/gic_monitors.csv'
    loc_df, latitude_dict = get_loc_df(loc_file)

    # Create list of file paths for each device_id in the event
    file_paths = []
    file_names = [f for f in os.listdir(event_dir) if f.startswith(str(eventDate)[0:4])]

    for file_name in os.listdir(event_dir):
        if file_name.startswith(str(eventDate)[0:4]):
            file_paths.append(event_dir + '/' + file_name)
    print(f"Number of Files for Event: {len(file_paths)}")

    # View example of file path
    # print(f"file_paths[0]: '{file_paths[0]}'")
    # print(f"file_paths: {file_paths}")
    # print(f"file names: {file_names}")
    # print(loc_df)

    #**************************************************************************
    # Initialize wavelet transform wavelet_results dictionary
    wavelet_results = {}

    # Other associated variables
    time_column = 'SampleDateTime'
    gic_column = 'GICMeasured'

    # Create pathway for wavelet transform
    file_path_wavelet_transform = os.path.join(output_dir, str(wavelet_transform) + '_results_' + str(eventDate) + '.json')

    # Conduct Wavelet Transform
    if os.path.exists(file_path_wavelet_transform):
        print(f'\n Wavelet transform already conducted \n')
        print(f"\n file_path_wavelet_transform exists! \n")
        
        with open(file_path_wavelet_transform, 'r') as file:
            wavelet_results = json.load(file)
        print(f"\nfile_path_wavelet_transform + loaded as json\n")
    else:
        print("running wavelet transform...")
        # For each device in the given event
        for file_name in file_names:
            file_path = os.path.join(event_dir, file_name)
            df = pd.read_csv(file_path)
            df[time_column] = pd.to_datetime(df[time_column], format='%m/%d/%Y %I:%M:%S %p')
            df = df.drop_duplicates(subset=time_column).set_index(time_column)
            df = df.reindex(pd.date_range(start=df.index.min(), end=df.index.max(), freq='T'), method='nearest')
            resampled_df = df.resample('T').mean().interpolate(method='spline', order=3)
            gic_signal = make_length_even(resampled_df[gic_column].values)
            level = determine_max_level(len(gic_signal))     # get the max level of decomposition for given gic_signal
            tupl = modwt(gic_signal, 'db4', level)[0]   # conduct wavelet transform based on max level of decomposition
            tupl = (tupl[0].tolist(),tupl[1].tolist()) # some formatting conversions
            result = [tupl]
            file_name = file_name.split('/')[-1]  # get the last element of split
            wavelet_results[file_name] = result

        # Open the file in write mode
        with open(file_path_wavelet_transform, 'w') as json_file:
            # Serialize the dictionary to JSON and write it to the file
            [json.dump(wavelet_results, json_file)]
        print(f"\n Initial Wavelet Transform saved to: file_path_wavelet_transform \n")

    # # Print out wavelet transform results
    # print(f'\n(initial modwt results) \n # of device_ids in the event : {len(wavelet_results)}\n')
    # print("The length of wavelet transform coefficients of a device_id is:")
    # print(list(wavelet_results.values())[0])

    desired_length = 1622     # check if time length of each device checks out
    filtered_wavelet_results = {device_id: modwt_vals 
                              for device_id, modwt_vals in wavelet_results.items() if len(modwt_vals[0][0]) == desired_length}

    #**************************************************************************
    # Filter out wavelet transform results based on time intervals
    file_path_wavelet_transform = output_dir + '/edges_' + str(eventDate) + '.json'

    print('Filtered modwt results:')
    print(f"# of device_ids (or files) in the event: {len(filtered_wavelet_results.keys())}")
    # NOTE: 48 is a random index for device ID
    print(f"# of time-based coefficients per device in the event: {len(list(filtered_wavelet_results.values())[48][0][0])}")  

    # READ the device_id specific thresholds from threshold_find_n0.csv file
    file_path_specific_thresholds = output_dir + '/' + str(eventDate) + '_threshold_find_n0.csv'
    df_correlation_thresholds = pd.read_csv(file_path_specific_thresholds)
    df_correlation_thresholds['Station'] = filtered_wavelet_results.keys()
    print(f"\n Successfully read the {eventDate}_threshold_find_n0.csv")
    print(df_correlation_thresholds.head())

    #**************************************************************************
    # Create Edges based on filtered modwt results and its cross correlation using wavelet_cross_correlation (or np.corrcoef)
    edges = []

    device_specific_threshold = False

    if device_specific_threshold:
        file_path_wavelet_transform_edges = output_dir +'/edges_' + str(eventDate) + '_device_specific.json'
    else:
        file_path_wavelet_transform_edges = output_dir +'/edges_' + str(eventDate) + '.json'

    # Find the edges based on correlation_threshold
    if os.path.exists(file_path_wavelet_transform_edges):
        print(f'\n Filtered Edges Already Exists! \n')
        with open(file_path_wavelet_transform_edges, 'r') as file:
            edges = json.load(file)
        print(file_path_wavelet_transform_edges + ' loaded as json')
    
    else:
        print(f'\n Filtering edges... \n')
        for (device_id1, wt1), (device_id2, wt2) in itertools.combinations(filtered_wavelet_results.items(), 2): # NOTE: combinations 
            cross_corrs = wavelet_cross_correlation(wt1, wt2)
            # print(cross_corrs)
            if device_specific_threshold:
            # Get the minimum correlation threshold btw device_id1 and device_id2
                threshold_1 = df_correlation_thresholds[df_correlation_thresholds['Station'] == device_id1]['Closest Threshold'].values[0]
                threshold_2 = df_correlation_thresholds[df_correlation_thresholds['Station'] == device_id2]['Closest Threshold'].values[0]
                correlation_threshold = max(threshold_1,threshold_2)
            else:
                correlation_threshold = 0.99
            for corr in cross_corrs:
                if np.abs(corr) > correlation_threshold:
                    edges.append((device_id1, device_id2, corr))

        # Open the file in write mode
        with open(file_path_wavelet_transform_edges, 'w') as json_file:
            # Serialize the dictionary to JSON and write it to the file
            [json.dump(edges, json_file)]
        print(f"\n Edges saved to: {file_path_wavelet_transform_edges} \n")
        
    #**************************************************************************
    # Create nx.graph for displaying correlations (non-geographic)
    G = nx.Graph()

    processed_edges = []
    for edge in edges:
        node1, node2, weight = edge
        node1_label = node1.replace('2013E02_', '').replace('.csv', '')
        node2_label = node2.replace('2013E02_', '').replace('.csv', '')
        processed_edges.append((node1_label, node2_label, weight))

    # Add edges to the blank nx.graph()
    G.add_weighted_edges_from(processed_edges)

    # convert the selected_loc_df to pos
    selected_loc_df = loc_df[['Device ID','Latitude','Longitude']]  
    pos = {}
    for index, row in selected_loc_df.iterrows():
        device_id = row['Device ID']
        lat = row['Latitude']
        lon = row['Longitude']
        pos[str(int(device_id))] = [lon,lat]

    weights = [G[u][v]['weight'] for u, v in G.edges()]

    nx.draw(G, pos, width=weights, with_labels=True, node_color='lightblue',
            edge_color='gray', node_size=50, alpha=0.6, font_size=8)

    plt.title('GIC Network Cross Correlation')

    if device_specific_threshold:
        plt.savefig(output_dir + '/edges_' + str(eventDate) + '_device_specific')
    else:
        plt.savefig(output_dir + '/edges_' + str(eventDate))
    # print('created ' + main_dir + '/output/' + str(eventDate) + '/correlations'+str(eventDate)+'/edges_' + str(eventDate))
    # This diagram shows a network diagram where the nodes represent different monitors.
    # Lines between nodes show correlations between different data sets.
    # The more nodes are wired, the higher the correlation with the other data sets.

    #**************************************************************************
    # TIME BASED CROSS CORRELATION ANALYSIS BY DORIA
    sliding_correlations = {}

    # Create path
    if device_specific_threshold:
        file_path_wavelet_transform_sliding_correlations = output_dir +'/sliding_correlations_' + str(eventDate) + '_' + str(window_size) + '_device_specific' + '.json'
    else:
        file_path_wavelet_transform_sliding_correlations = output_dir +'/sliding_correlations_' + str(eventDate) + '_' + str(window_size) + '.json'

    # Find the edges based on correlation_threshold
    if os.path.exists(file_path_wavelet_transform_sliding_correlations):
        print(f'\nFiltered Sliding Correlation Already Exists\n')
        with open(file_path_wavelet_transform_sliding_correlations, 'r') as file:
            sliding_correlations = json.load(file)
            sliding_correlations =  {tuple(eval(k)): v for k, v in sliding_correlations.items()} # convert string keys to tuple keys
        print(file_path_wavelet_transform_edges + ' loaded as json')
    else:
        print(f'\nFiltered Sliding Correlations with window size: {str(window_size)}\n')
        # NOTE: itertools.combinations compares each tuple or key-value pair with every other tuple or key-value pair
        for (file1, wt1), (file2, wt2) in itertools.combinations(filtered_wavelet_results.items(), 2):
            corrs_over_time = sliding_window_cross_correlation(wt1[0][0], wt2[0][0], window_size)
            print('***') # to see if loop is running
            sliding_correlations[(file1, file2)] = corrs_over_time

        # Need to convert sliding_correlations keys to str (not tuple)
        # Convert tuples to strings
        converted_dict = {str(k): v for k, v in sliding_correlations.items()}

        # Open the file in write mode
        with open(file_path_wavelet_transform_sliding_correlations, 'w') as json_file:
            # Serialize the dictionary to JSON and write it to the file
            [json.dump(converted_dict, json_file)]
        print(f'\nSliding Correlations saved to: {file_path_wavelet_transform_sliding_correlations}\n')

    # print(sliding_correlations)

    # print(f"# of Device Combinations: {len(sliding_correlations.keys())}")
    # print(f"# of Device Combinations Values: {len(sliding_correlations.values())}")
    # print(f"# of Device Combinations Values (Time Series) Coefficient: {len(list(sliding_correlations.values())[35])}")

    # TIME BASED // Written by Doria
    # Getting the time axis
    file_pair_to_visualize = (file_names[0], file_names[9]) # dummy input
    correlations_over_time = sliding_correlations[file_pair_to_visualize]
    time_axis = range(len(correlations_over_time))

    print(f'time_axis: {time_axis}')
    #**************************************************************************
    print('Creating each graph for time slot')
    G_graphs = [[] for i in range(len(correlations_over_time))]

    if device_specific_threshold:
        file_path_wavelet_transform_sliding_correlations_graphs = output_dir +'/sliding_correlations_graphs' + str(eventDate) + '_' + str(window_size) + '_device_specific' '.json'
    else:
        file_path_wavelet_transform_sliding_correlations_graphs = output_dir +'/sliding_correlations_graphs' + str(eventDate) + '_' + str(window_size) + '.json'

    if os.path.exists(file_path_wavelet_transform_sliding_correlations_graphs):
        print(f'\G_graphs already exists!\n')
        with open(file_path_wavelet_transform_sliding_correlations_graphs, 'r') as file:
            G_graphs = json.load(file)

        print(file_path_wavelet_transform_sliding_correlations_graphs + ' loaded as json')
    
    else:
        print(f'\nCreating graph for each time slot (window_size={str(window_size)})\n')

        for (device_id1, device_id2), correlations in sliding_correlations.items():
            i = 0
            while i < len(time_axis):  # NOTE: NEED TO CHANGE THIS NUMBER
                # print(i, correlations[i])
                # print(cross_corrs)
                if device_specific_threshold:
                # Get the minimum correlation threshold btw device_id1 and device_id2
                    threshold_1 = df_correlation_thresholds[df_correlation_thresholds['Station'] == device_id1]['Closest Threshold'].values[0]
                    threshold_2 = df_correlation_thresholds[df_correlation_thresholds['Station'] == device_id2]['Closest Threshold'].values[0]
                    correlation_threshold = max(threshold_1,threshold_2)
                else:
                    correlation_threshold = 0.99
                if abs(correlations[i]) > correlation_threshold:
                    G_graphs[i].append((device_id1, device_id2, correlations[i]))
                i += 1
            print('graph') # to see if loop is running
            
        # Open the file in write mode
        with open(file_path_wavelet_transform_sliding_correlations_graphs, 'w') as json_file:
            # Serialize the dictionary to JSON and write it to the file
            [json.dump(G_graphs, json_file)]
        print(f'\nSliding Correlations Graphs saved!\n')


    #**************************************************************************
    # This diagram shows a network diagram where the nodes represent different monitors.
    # Lines between nodes show correlations between different data sets.
    # The more nodes are wired, the higher the correlation with the other data sets.

    # TIME BASED Cross Correlation // Written by Sean Jung
     # Initialize frames
    frames = []
    frames_centrality = []
    # print(G_graphs)

    print(f"startTime: {startTime}")
    print(f"endTime: {endTime}")

    print(f"G_graphs length: {len(G_graphs[startTime:endTime][::60])}")
    # Iterate over time points
    for i, graph in enumerate(G_graphs[startTime:endTime][::60]):
        G = nx.Graph()   # initialize new graph
        processed_edges = []

        # Process edges for the current graph
        for edge in graph:
            node1, node2, weight = edge
            node1_label = node1.replace('2013E02_', '').replace('.csv', '')
            node2_label = node2.replace('2013E02_', '').replace('.csv', '')
            processed_edges.append((node1_label, node2_label, weight))

        G.add_weighted_edges_from(processed_edges)
        
        # Centrality measures
        # closeness_centrality = nx.closeness_centrality(G)
        # betweenness_centrality = nx.betweenness_centrality(G)
        eigenvector_centrality = nx.eigenvector_centrality(G)

        # Initialize node and edge traces for each graph
        node_traces = []
        edge_traces = []

        # Extract node positions of given graph
        node_x = []
        node_y = []
        for node in G.nodes():
            y, x = lookup_lat_long(node, loc_df)
            node_x.append(x)
            node_y.append(y)

        # Create node trace
        node_trace = go.Scattermapbox(
            lon=node_x,
            lat=node_y,
            mode='markers',
            showlegend=False,
            marker=dict(
                size=5,
                color='red',
            ),
            hoverlabel=dict(bgcolor='white', font=dict(color='black'))
        )
        node_traces.append(node_trace)
        
        # Extract edge positions
        for u, v, w in G.edges(data=True):
            y0, x0 = lookup_lat_long(u, loc_df)
            y1, x1 = lookup_lat_long(v, loc_df)

            edge_trace = go.Scattermapbox(
                lon=[x0, x1],
                lat=[y0, y1],
                line=dict(width=1,
                        color='blue'),
                mode='lines',
                name=f"{w['weight']:.2f}",
                text=f"{w['weight']:.2f}",
                showlegend=False,
            )
            edge_traces.append(edge_trace)

        centrality_node_traces = []
        centrality_node_trace = go.Scattermapbox(
            lon=node_x,
            lat=node_y,
            mode='markers',
            showlegend=False,
            marker=dict(
                size=[x*20 for x in eigenvector_centrality.values()],
                color='red',
            ),
            hoverlabel=dict(bgcolor='white', font=dict(color='black'))
        )
        centrality_node_traces.append(centrality_node_trace)
        # Add traces to frames
        frames.append(go.Frame(data=node_traces + edge_traces, name=str(i)))
        frames_centrality.append(go.Frame(data=edge_traces + centrality_node_traces, name=str(i)))

    # Create buttons for Play and Pause
    updatemenus = [
        dict(
            type="buttons",
            buttons=[
                dict(label="Play",
                    method="animate",
                    args=[None, {"frame": {"duration": 500}, "fromcurrent": True}]),
                dict(label="Pause",
                    method="animate",
                    args=[[None], {"frame": {"duration": 0, "redraw": False}, "mode": "immediate"}])
            ],
            x=0,
            xanchor='left',
            y=0,
            yanchor='bottom'
        )
    ]

    sliders=[{
        'active': 0,
        'yanchor': 'top',
        'xanchor': 'left',
        'currentvalue': {
            'xanchor': 'right'
        },
        'len': 0.9,
        'x': 0.1,
        'y': 0,
        'steps': [{'args': [[frame.name], {'frame': {'duration': 300, 'redraw': True}, 'mode': 'immediate'}],
                   'label': frame.name,
                   'method': 'animate'} for frame in frames]
    }]

    # Create layout
    layout = go.Layout(
        title='Wavelet Transform Coefficient X-Correlation // ',
        mapbox=dict(
            style="carto-positron",
            zoom=1.5,
            center=dict(lat=37.0902, lon=-98.7129)
        ),
        updatemenus=updatemenus,
        sliders=sliders,
        xaxis=dict(range=[-130, -60], scaleanchor="y", scaleratio=1, dtick=2),
        xaxis_title='Longitude',
        yaxis=dict(range=[25, 55], scaleanchor="x", scaleratio=1, dtick=2),
        yaxis_title='Latitude',
    )

    # Create Plotly figure
    fig = go.Figure(data=edge_traces+node_traces,layout=layout, frames=frames)

    # CREATE CENTRALITY MEASURES GRAPH
    fig4 = go.Figure(data=edge_traces+node_traces,layout=layout,frames=frames_centrality)
    fig4.update_layout(title= 'w/ EigenVector centrality measure')

    # Centrality measures fig

    return fig, fig4

