![cme0](../static/images/cme0.png)
>*Credit: NASA Visualization Studio*

# Abstract

A Coronal Mass Ejection (CME) is a solar phenomenon that discharges an enormous cloud of highly magnetized plasma into space causing significant magnetic fluctuations on Earthâ€™s magnetosphere. Historically, these fluctuations cause geomagnetically-induced currents (GIC) that cause major damage to critical, global infrastructure including electrical transformers, oil pipelines, and long-distance telecommunication lines. Accurately predicting the frequency, characteristics, and effects of GICs requires deep network and extreme value analysis, pre-processing gigabytes of data under a deliberate analysis-ready framework. Thus, in response, this research has created a comprehensive data pipeline that aggregates the GIC measurements of major datasets on the public domain(i.e SuperMAG, OMNIWeb, NERC), structuring schemas to facilitate network and extreme value analysis (EVA). Under EVA, Generalized Extreme Value Model outlines estimates 1-in-100 year catastrophic solar events, resulting in an inversely, non-linear trend between return values and pre-defined return period (T). Meanwhile, Quantile-Quantile plots indicate the characteristics of regions, differentiating between supernodes of greater GIC activity by magnitude. Under network analysis, a wavelet decomposition was conducted to identify long and short-term trends, utilizing wavelet coefficients to find cross-correlation with threshold optimization, solidifying previous observations of the long east-west connections (Orr 2021). Overlaying a synthetic physical grid to visualize the potential impact of GIC variations provided uninteresting conclusions, yet, applying eigenvalue centrality measures highlighted previously unforeseen areas of interests - beyond active regions surrounding super-nodes. Predictions of these GIC events and magnetic perturbations were modeled using three machine learning (ML) models, including multi-linear regressions (MLR), random forest regression (RFR), and long-short term model (LSTM). Given a 5-min interval events, LSTM had the best performance in modeling using ReLu activation and Root Means Square Error (RMSE) loss function with  Adam optimization. Batch size was set at 360, since smaller batch size induces a slower and less stable training process, while a larger batch size led to poorer generalization. 
